from tqdm import tqdm

import torch
from torch.utils.data import DataLoader
from transformers import BertJapaneseTokenizer, BertForSequenceClassification

import tensorflow as tf
import pytorch_lightning as pl

import csv
import pandas as pd
import numpy as np
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from transformers import pipeline, AutoModelForSequenceClassification, BertJapaneseTokenizer,BertTokenizer, BertForSequenceClassification
import torch.nn.functional as F


from preprocessing_japanese import preprocess

####################################### ã‚³ãƒ¡ãƒ³ãƒˆå…¥åŠ›ã€å‰å‡¦ç† #######################################

text = preprocess().text_cleaning(input('ã‚³ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã—ã¦ä¸‹ã•ã„ > ')) 




####################################### æ—¢å­˜ã®BERTãƒ¢ãƒ‡ãƒ«ã§æ„Ÿæƒ…åˆ†æ #######################################

model = AutoModelForSequenceClassification.from_pretrained("koheiduck/bert-japanese-finetuned-sentiment")
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
classifier = pipeline("sentiment-analysis",model=model,tokenizer=tokenizer)
result = classifier(text)[0]  #ã‚³ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›
bert_sentiment_score  = round(result['score'], 4) #æ—¢å­˜ã®BERTãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
bert_sentiment_label = result['label'] #æ—¢å­˜ã®BERTãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«
if bert_sentiment_label == "POSITIVE":
    bert_sentiment_label = 1
elif bert_sentiment_label == "NEGATIVE":
    bert_sentiment_label = 2
else :
    bert_sentiment_label = 3




######################################## æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®2å€¤åˆ†é¡ #######################################

MODEL_NAME = '/home0/y2020/s2010320/Sarcasm_detection/MHA-BiLSTM/model_transformers_stance_manual/0.8294481847503103' #æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®2å€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
bert_sc = BertForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=2
)

# å„ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’æ•´ãˆã‚‹
max_length = 128
dataset_for_loader = []

text_list =[]
label_list = []
text_list = [text]


encoding = tokenizer(
    text_list,
    max_length=max_length,
    padding='max_length',
    truncation=True
)
encoding = { k: torch.tensor(v) for k, v in encoding.items() }

# æ¨è«–
with torch.no_grad():
    output = bert_sc.forward(**encoding)
scores = output.logits # åˆ†é¡ã‚¹ã‚³ã‚¢
labels_predicted_sentiment = scores.argmax(-1) # ã‚¹ã‚³ã‚¢ãŒæœ€ã‚‚é«˜ã„ãƒ©ãƒ™ãƒ«






######################################## çš®è‚‰ã®2å€¤åˆ†é¡(æ—¢å­˜æ‰‹æ³•ã®ã¿) #######################################

MODEL_NAME = '/home0/y2020/s2010320/Sarcasm_detection/MHA-BiLSTM/model_final_F1/0.6666666666666666' #final.py best

tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
bert_sc = BertForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=2
)
#bert_sc = bert_sc.cuda()

# å„ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’æ•´ãˆã‚‹
max_length = 128
dataset_for_loader = []

text_list =[]
label_list = []
text_list = [text]

encoding = tokenizer(
    text_list,
    max_length=max_length,
    padding='max_length',
    truncation=True
)
encoding = { k: torch.tensor(v) for k, v in encoding.items() }
#label_list = torch.tensor(label_list).cuda()

# æ¨è«–
with torch.no_grad():
    output = bert_sc.forward(**encoding)
scores = output.logits # åˆ†é¡ã‚¹ã‚³ã‚¢
labels_predicted = scores.argmax(-1)


######################################## ææ¡ˆæ‰‹æ³•(æ„Ÿæƒ…ã€ãƒ©ãƒ™ãƒ«äºˆæ¸¬ç¢ºç‡) #######################################

# ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã®é©ç”¨
scores = F.softmax(scores, dim=1)
scores.tolist()
x = scores[0]
if bert_sentiment_label == 1 and labels_predicted_sentiment == 1 : #BERTã«ã‚ˆã‚‹æ„Ÿæƒ…åˆ†æã®çµæœãŒãƒã‚¸ãƒ†ã‚£ãƒ–ã‹ã¤ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸæ„Ÿæƒ…åˆ†æ(2å€¤åˆ†é¡)ã®çµæœãŒãƒã‚¬ãƒ†ã‚£ãƒ–
        if labels_predicted == 0 and bert_sentiment_score > 0.9 and float(x[0]) < 0.9: #ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’ä½¿ã£ãŸæ™‚
            labels_predicted = 1




########################################  ææ¡ˆæ‰‹æ³•(ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢) #######################################

#å„å˜èªã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ç®—å‡ºã«bertãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
from transformers import pipeline, AutoModelForSequenceClassification, BertJapaneseTokenizer,BertTokenizer, BertForSequenceClassification
from transformers import AutoTokenizer
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™
model = AutoModelForSequenceClassification.from_pretrained("koheiduck/bert-japanese-finetuned-sentiment")
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
classifier = pipeline("sentiment-analysis",model=model,tokenizer=tokenizer)

tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking').tokenize

paradox_words = ["é€†ã«", "ãã‚ƒãã«" "ã•ã™ãŒ", "æµçŸ³", "ã‚€ã—ã‚", "å¯§ã‚", "æ–¹ãŒ", "ã»ã†ãŒ","è‰¯ã", "ã‚ˆã", "ä½è©•ä¾¡", "ğŸ‘", "ãƒãƒƒãƒ‰", "Bad", "bad", "æš´åŠ›"]
final_words = ["w", "ç¬‘", "?", "^ ^", "^ ^", "ã‚ã‚ãŸ", "ãƒ¯ãƒ­ã‚¿", "ğŸ¤ª", "ğŸ˜Š", "ğŸ˜€", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜†", "ğŸ˜", "ğŸ˜‚", "ğŸ¤£"]

sentence_score = 0 #ã¨ã‚Šã‚ãˆãšãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚’ï¼ã«ã—ã¨ã
final_words_score = 0 #ã¨ã‚Šã‚ãˆãšæ–‡æœ«å˜èªã‚¹ã‚³ã‚¢ã‚’ï¼ã«ã—ã¨ã
for keyword in paradox_words:
    if keyword in text: #é€†èª¬å˜èªã®ã†ã¡ã„ãšã‚Œã‹ãŒæ–‡ã«å«ã¾ã‚Œã¦ã„ãŸã‚‰
        part_of_text = text.split(keyword, 1) #é€†èª¬å˜èªã®å¾Œã‚’åˆ‡ã‚ŠæŠœã
        #print(part_of_text[1])
        result = classifier(part_of_text[1])[0] #åˆ‡ã‚ŠæŠœã„ãŸéƒ¨åˆ†ã‚’æ„Ÿæƒ…åˆ†æ
        if result["label"] == "POSITIVE": #åˆ‡ã‚ŠæŠœã„ãŸéƒ¨åˆ†ãŒãƒã‚¸ãƒ†ã‚£ãƒ–ã ã£ãŸã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
            sentence_score = round(result["score"], 5) #round()é–¢æ•°ã‚’ä½¿ã£ã¦å°æ•°ç‚¹ä»¥ä¸‹ã®æ¡æ•°ã‚’5æ¡ã«æŒ‡å®š
for final_keyword in final_words: #æ–‡æœ«å˜èªã®ã†ã¡ã„ãšã‚Œã‹ãŒä¸ãˆã‚‰ã‚ŒãŸæ–‡ã®æ–‡æœ«å˜èªã¨ä¸€è‡´ã—ãŸã‚‰
    if final_keyword == text[-1]:
        final_words_score = 1
pattern_matching_score = sentence_score*final_words_score
if pattern_matching_score > 0 and labels_predicted_sentiment == 1: #ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ãŒ0ä»¥ä¸Šã‹ã¤æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãŒ1(ãƒã‚¬ãƒ†ã‚£ãƒ–)ã ã£ãŸã‚‰äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’1ã«ã™ã‚‹
    labels_predicted = 1

print()

if labels_predicted == 1:
    print('çš®è‚‰ã§ã™ !')
elif labels_predicted == 0:
    print('çš®è‚‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ !')

print()