import numpy as np
import torch
import pandas as pd
import csv
from sklearn.model_selection import train_test_split
from torch.autograd import Variable
import re
import os
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
SEED = 1234
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
from torch.utils.data import DataLoader, Dataset, TensorDataset
import nltk
#nltk.download('punkt')
from nltk.tokenize import word_tokenize
import MeCab
from torch import tensor
from torch import int32, float32
from pprint import pprint
import time
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import dill
import pickle
from transformers import AutoTokenizer
import MeCab
mecab = MeCab.Tagger("-Owakati")

#å„å˜èªã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ç®—å‡ºã«bertãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
from transformers import pipeline, AutoModelForSequenceClassification, BertJapaneseTokenizer,BertTokenizer, BertForSequenceClassification

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™
model = AutoModelForSequenceClassification.from_pretrained("koheiduck/bert-japanese-finetuned-sentiment")
#model = AutoModelForSequenceClassification.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")  
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
classifier = pipeline("sentiment-analysis",model=model,tokenizer=tokenizer)






dataset_path = rootDir="/Users/satogo/Downloads/ç ”ç©¶/çš®è‚‰/Sarcasm-Detection-master/ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/data2_preprocessed.csv"
sentiment_dict_path = '/Users/satogo/Downloads/ç ”ç©¶/çš®è‚‰/Sarcasm-Detection-master/SVM/Bush/wordlist_japanese.csv'

tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking').tokenize

sentiment_words = []
sentiment_words_hiragana = []
sentiment_scores = []
df = pd.read_csv(sentiment_dict_path, sep=',')
for line in df.values:
   sentiment_words.append(line[0]) #è¾æ›¸ã«å«ã¾ã‚Œã‚‹ã‚¹ã‚³ã‚¢ã¤ãã®å˜èª(æ¼¢å­—)
   sentiment_words_hiragana.append(line[1]) #è¾æ›¸ã«å«ã¾ã‚Œã‚‹ã‚¹ã‚³ã‚¢ã¤ãã®å˜èª(ã²ã‚‰ãŒãªã®ã¿)
   sentiment_scores.append(line[3]) #è¾æ›¸ã«å«ã¾ã‚ŒãŸã‚¹ã‚³ã‚¢
   

#print(sentiment_words[1])

review_texts = []
labels = []
ids = []
tokens_list = []

df = pd.read_csv(dataset_path, sep=',')
for line in df.values:
   token_list = []
   review_text = line[0]   # ãƒ†ã‚­ã‚¹ãƒˆã¯1åˆ—ç›®
   review_texts.append(review_text)
   # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒªã‚¹ãƒˆã«æ ¼ç´
   token_list = mecab.parse(review_text).split(" ")
   del token_list[-1] ##æœ€å¾Œã«'\n'ãŒå…¥ã£ã¡ã‚ƒã£ã¦ã‚‹ã‹ã‚‰ãã‚Œã‚’å‰Šé™¤
   label = line[1] #ãƒ©ãƒ™ãƒ«ã¯2åˆ—ç›®
   labels.append(label)
   id = line[2] #idã¯3åˆ—ç›®
   ids.append(id)

   print(token_list)

   ##words = words.split('ã€€')
   tokens_list.append(token_list) #ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãŸæ–‡ç« ã®å„å˜èªã‚’é…åˆ—ã«æ ¼ç´


sentiment_words_list_list = []  #ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆä¸­ã®å„å˜èªã‚’é…åˆ—ã«æ ¼ç´
sentiment_scores_list_list = [] #ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆä¸­ã®å„å˜èªã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’é…åˆ—ã«æ ¼ç´

paradox_words = ["é€†", "ãã‚ƒã" "ã•ã™ãŒ", "æµçŸ³", "ã™ã”ã„", "ã™ã”ã„ãª", "å‡„ã„", "å‡„ã„ãª", "ã™ã’ãˆ","ã™ã’ã‡", "ã™ã’ãƒ¼", "ã‚€ã—ã‚", "å¯§ã‚", "æ–¹", "ã»ã†","è‰¯ã", "ã‚ˆã", "w", "ç¬‘", "?","ğŸ˜‚"]
final_words = ["w", "ç¬‘", "?", "ğŸ˜‚"]
paradox_scores_list_list = []


max_sentiment_words_number = 0
max_paradox_words_number = 0
paradox_words_position = 0
pattern_matching_list = [] #ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’0or1ã§åˆ¤æ–­ã™ã‚‹é…åˆ—
final_words_score_list = [] #æ–‡æœ«å˜èªãŒæŒ‡å®šå˜èªã§çµ‚ã‚ã£ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’0or1ã§åˆ¤æ–­ã™ã‚‹é…åˆ—

for tokens in tokens_list:
    sentiment_words_number = 0
    paradox_words_number = 0
    sentiment_words_list = []  #ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆä¸­ã®å„å˜èªã‚’é…åˆ—ã«æ ¼ç´
    sentiment_scores_list = []  #ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆä¸­ã®å„å˜èªã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã‚’é…åˆ—ã«æ ¼ç´
    paradox_scores_list =[] #é€†èª¬ã«ãªã‚‹ã‚ˆã†ãªå˜èªã‚’å«ã‚“ã§ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹é…åˆ—
    pattern_matching = 0
    paradox_words_position = -1 #oã®ä½ç½®ã«é€†èª¬å˜èªãŒã‚ã‚‹å ´åˆã‚’è€ƒæ…®ã—ã¦é€†èª¬å˜èªãŒãªã„é–“ã¯å€¤ã‚’-1ã«
    i = 0
    for token in tokens:
        sentiment_word = token
        result = classifier(token)[0]
        sentiment_score = result["score"]
        if result["label"] == "NEUTRAL":
            sentiment_score = 0
        elif result["label"] == "NEGATIVE":
            sentiment_score = -result["score"]

        if result["label"] == "POSITIVE" and i > paradox_words_position and paradox_words_position != -1:
            sentiment_word = token
            #pattern_matching = 1 
            pattern_matching = result["score"]

        sentiment_words_list.append(sentiment_word)
        if sentiment_score != 0: #æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãŒ0ã ã£ãŸã‚‰ãƒªã‚¹ãƒˆã«ã¯æ ¼ç´ã—ãªã„
            sentiment_scores_list.append(sentiment_score)
            sentiment_words_number += 1

        if token in paradox_words: #é€†èª¬ã®å˜èªãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰
            paradox_score = 1
            paradox_words_position = i #é€†èª¬ãŒå«ã¾ã‚Œã¦ã„ã‚‹å˜èªã®ä½ç½®ã‚’ç‰¹å®š
        else:
            paradox_score = 0
        paradox_scores_list.append(paradox_score)
        paradox_words_number += 1 
        i += 1

    if sentiment_words_number > max_sentiment_words_number: #æ„Ÿæƒ…æœ‰ã‚Šã®å˜èªæ•°ã®æœ€å¤§é•·ã‚’èª¿ã¹ã‚‹
        max_sentiment_words_number = sentiment_words_number
    if paradox_words_number > max_paradox_words_number: #ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãŸå˜èªæ•°ã®æœ€å¤§é•·ã‚’èª¿ã¹ã‚‹
        max_paradox_words_number = paradox_words_number
    sentiment_words_list_list.append(sentiment_words_list)
    sentiment_scores_list_list.append(sentiment_scores_list)
    paradox_scores_list_list.append(paradox_scores_list)
    pattern_matching_list.append(pattern_matching)
    if sentiment_words_list[-1] in final_words: #æ–‡æœ«å˜èªãŒæŒ‡å®šå˜èªã§çµ‚ã‚ã£ã¦ã„ãŸã‚‰1ã‚’ã€ãã‚Œä»¥å¤–ã¯0ã‚’ä»£å…¥
        final_words_score_list.append(1)
    else:
        final_words_score_list.append(0)

print("\n")

for i in sentiment_scores_list_list:    
    print(i)
    print("\n")
    
# å‡ºåŠ›çµæœã‚’CSVã«ä¿å­˜
#ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›

shift_columns = 1
with open("æ–‡ç« ä¸­ã«å«ã¾ã‚Œã‚‹å„å˜èªã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢(bert).csv", "w", encoding="utf-8", newline="") as f: #csvãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ã
    writer = csv.writer(f)
    header = []
    for i in range(1 + max_sentiment_words_number + max_paradox_words_number + 3 + 2 + 2 + 4): #å¿…è¦ãªæ•°ã ã‘åˆ—ã‚’ç¢ºä¿
        header.append("")  # ç©ºç™½ã®åˆ—ã‚’æŒ¿å…¥
    header[0] = "ã‚³ãƒ¡ãƒ³ãƒˆ"
    header[1] = ""
    header[2] = "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"
    i = 3
    while(i < max_sentiment_words_number+3):
        header[i] = ""
        i += 1
    header[i] = "é€†èª¬ã‚¹ã‚³ã‚¢"
    i+=1
    while(i < max_sentiment_words_number+3+max_paradox_words_number):
        header[i] = ""
        i += 1
    i+=1
    header[i] = "ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"
    header[i+1] = ""
    header[i+2] = "æ–‡æœ«å˜èª"
    header[i+3] = ""
    header[i+4] = "æ­£è§£ãƒ©ãƒ™ãƒ«"
    header[i+5] = ""
    header[i+6] = "id"
    writer.writerow(header)

    # å„è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ãšã‚‰ã—ã¦æ›¸ãè¾¼ã‚€
    for c, scores_list, paradox_score_list, pattern_matching, final_words, label, id in zip(review_texts, sentiment_scores_list_list, paradox_scores_list_list, pattern_matching_list, final_words_score_list, labels, ids):
        shifted_row = [c] + [None] * shift_columns + scores_list + [None] *(max_sentiment_words_number - len(scores_list) + 1)  + paradox_score_list + [None] *(max_paradox_words_number - len(paradox_score_list) + 1) + [pattern_matching] + [None] * shift_columns + [final_words] + [None]+ [label] + [None] + [id]
        writer.writerow(shifted_row)
        #for paradox_score in paradox_score_list:
        #    if paradox_score == 1:

