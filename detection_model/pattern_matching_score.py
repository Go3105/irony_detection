#é€†èª¬å˜èªã‚’è¦‹ã¤ã‘ãŸã‚‰ãã‚Œä»¥é™ã®æ–‡ã‚’åˆ‡ã‚ŠæŠœã„ã¦bertã§æ„Ÿæƒ…åˆ†æã‚’è¡Œã„ã€
#ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºã‹ã‚ã‚‹æ–¹æ³•

import decimal
import numpy as np
import torch
import pandas as pd
import csv
from sklearn.model_selection import train_test_split
from torch.autograd import Variable
import re
import os
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
SEED = 1234
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
from torch.utils.data import DataLoader, Dataset, TensorDataset
#nltk.download('punkt')
import MeCab
from torch import tensor
from torch import int32, float32
from pprint import pprint
import time
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import dill
import pickle
from transformers import AutoTokenizer
import MeCab
mecab = MeCab.Tagger("-Owakati")


#å„å˜èªã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ç®—å‡ºã«bertãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
from transformers import pipeline, AutoModelForSequenceClassification, BertJapaneseTokenizer,BertTokenizer, BertForSequenceClassification

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™
model = AutoModelForSequenceClassification.from_pretrained("koheiduck/bert-japanese-finetuned-sentiment")
#model = AutoModelForSequenceClassification.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")  
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')
classifier = pipeline("sentiment-analysis",model=model,tokenizer=tokenizer)

#sentiment_dict_path = '/Users/satogo/Downloads/ç ”ç©¶/çš®è‚‰/Sarcasm-Detection-master/SVM/Bush/wordlist_japanese.csv'

tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking').tokenize

review_texts = []
labels = []
ids = []

df = pd.read_csv("/home0/y2020/s2010320/Sarcasm_detection/MHA-BiLSTM/pattern_matching_score(bert_sentence).csv", sep=',')
for line in df.values:
   review_text = line[0]   # ãƒ†ã‚­ã‚¹ãƒˆã¯1åˆ—ç›®
   review_texts.append(review_text)
   #label = line[1] #ãƒ©ãƒ™ãƒ«ã¯2åˆ—ç›®
   #labels.append(label)
   #id = line[2] #idã¯3åˆ—ç›®
   #ids.append(id)

paradox_words = ["é€†ã«", "ãã‚ƒãã«" "ã•ã™ãŒ", "æµçŸ³", "ã‚€ã—ã‚", "å¯§ã‚", "æ–¹ãŒ", "ã»ã†ãŒ","è‰¯ã", "ã‚ˆã", "ä½è©•ä¾¡", "ğŸ‘", "ãƒãƒƒãƒ‰", "Bad", "bad", "æš´åŠ›"]
final_words = ["w", "ç¬‘", "?", "ï¼Ÿ", "^ ^", "^ ^", "ã‚ã‚ãŸ", "ãƒ¯ãƒ­ã‚¿", "ğŸ¤ª", "ğŸ˜Š", "ğŸ˜€", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜†", "ğŸ˜", "ğŸ˜‚", "ğŸ¤£"]
sentence_score_list = []
pattern_matching_score_list = []
final_words_score_list = []


for text in review_texts:
    sentence_score = 0 #ã¨ã‚Šã‚ãˆãšãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚’ï¼ã«ã—ã¨ã
    final_words_score = 0 #ã¨ã‚Šã‚ãˆãšæ–‡æœ«å˜èªã‚¹ã‚³ã‚¢ã‚’ï¼ã«ã—ã¨ã
    for keyword in paradox_words:
        if keyword in text: #é€†èª¬å˜èªã®ã†ã¡ã„ãšã‚Œã‹ãŒæ–‡ã«å«ã¾ã‚Œã¦ã„ãŸã‚‰
            part_of_text = text.split(keyword, 1) #é€†èª¬å˜èªã®å¾Œã‚’åˆ‡ã‚ŠæŠœã
            #print(part_of_text[1])
            result = classifier(part_of_text[1])[0] #åˆ‡ã‚ŠæŠœã„ãŸéƒ¨åˆ†ã‚’æ„Ÿæƒ…åˆ†æ
            if result["label"] == "POSITIVE": #åˆ‡ã‚ŠæŠœã„ãŸéƒ¨åˆ†ãŒãƒã‚¸ãƒ†ã‚£ãƒ–ã ã£ãŸã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
                sentence_score = round(result["score"], 5) #round()é–¢æ•°ã‚’ä½¿ã£ã¦å°æ•°ç‚¹ä»¥ä¸‹ã®æ¡æ•°ã‚’5æ¡ã«æŒ‡å®š
    sentence_score_list.append(sentence_score)
    for final_keyword in final_words: #æ–‡æœ«å˜èªã®ã†ã¡ã„ãšã‚Œã‹ãŒä¸ãˆã‚‰ã‚ŒãŸæ–‡ã®æ–‡æœ«å˜èªã¨ä¸€è‡´ã—ãŸã‚‰
        if final_keyword == text[-1]:
            final_words_score = 1
    final_words_score_list.append(final_words_score)
    pattern_matching_score = sentence_score*final_words_score
    pattern_matching_score_list.append(pattern_matching_score)
        
df = pd.read_csv("/home0/y2020/s2010320/Sarcasm_detection/MHA-BiLSTM/pattern_matching_score(bert_sentence).csv", sep=',')
df["ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°(åˆè¨ˆ)ã‚¹ã‚³ã‚¢"] = pattern_matching_score_list
#df["æ„Ÿæƒ…ã‚¹ã‚³ã‚¢(bert)"] = df["æ„Ÿæƒ…ã‚¹ã‚³ã‚¢(bert)"].fillna(0).astype(np.int64)
df.to_csv('pattern_matching_score(bert_sentence).csv', index=None)